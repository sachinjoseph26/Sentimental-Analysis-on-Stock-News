# Tokenization
df['News Headline'] = df['News Headline'].apply(lambda x: word_tokenize(x))

# Stop-word removal
stop_words = set(stopwords.words('english'))
df['News Headline'] = df['News Headline'].apply(lambda x: [word for word in x if word.lower() not in stop_words])

# Stemming
porter = PorterStemmer()
df['News Headline'] = df['News Headline'].apply(lambda x: [porter.stem(word) for word in x])

# Lemmatization
lemmatizer = WordNetLemmatizer()
df['News Headline'] = df['News Headline'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])

# Lowercasing
# Lowercasing is usually done before other preprocessing steps, but we can do it here as well
# Convert all words to lowercase
# df['News Headline'] = df['News Headline'].apply(lambda x: [word.lower() for word in x])



import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from gensim.models import Word2Vec
from sentence_transformers import SentenceTransformer


# Bag-of-Words representation
count_vectorizer = CountVectorizer()
bow_matrix = count_vectorizer.fit_transform(df['News Headline'].apply(lambda x: ' '.join(x)))

# TF-IDF representation
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(df['News Headline'].apply(lambda x: ' '.join(x)))

# Word2Vec embedding
word2vec_model = Word2Vec(sentences=df['News Headline'], vector_size=100, window=5, min_count=1, workers=4)

# BERT embedding
bert_model = SentenceTransformer('bert-base-nli-mean-tokens')
bert_embeddings = bert_model.encode(df['News Headline'].apply(lambda x: ' '.join(x)))

print('Feature extraction techniques completed.')